{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "2DgpQPU9ZZKK"
      },
      "outputs": [],
      "source": [
        "# CS4990 Prompt Engineering\n",
        "# Assignment 4 - A Simple Question Answer Language Model\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "Tokenizer = tf.keras.preprocessing.text.Tokenizer\n",
        "# Correct import statements\n",
        "# Access keras models and layers through TensorFlow\n",
        "Dropout = tf.keras.layers.Dropout\n",
        "BatchNormalization = tf.keras.layers.BatchNormalization\n",
        "Sequential = tf.keras.models.Sequential\n",
        "Dense = tf.keras.layers.Dense\n",
        "Embedding = tf.keras.layers.Embedding\n",
        "Bidirectional = tf.keras.layers.Bidirectional\n",
        "LSTM = tf.keras.layers.LSTM\n",
        "pad_sequences = tf.keras.preprocessing.sequence.pad_sequences\n",
        "to_categorical = tf.keras.utils.to_categorical\n",
        "Adam = tf.keras.optimizers.Adam\n",
        "EarlyStopping = tf.keras.callbacks.EarlyStopping\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open and read the content of the Python file\n",
        "\n",
        "file_path = 'dataset.py'\n",
        "\n",
        "# Initialize an empty dictionary to hold variables defined in the file\n",
        "namespace = {}\n",
        "\n",
        "# Execute the file's contents within the provided namespace\n",
        "with open(file_path, 'r') as file:\n",
        "    exec(file.read(), namespace)\n",
        "\n",
        "# Access the list from the namespace where it was executed\n",
        "scientific_facts_part1 = namespace.get('scientific_facts_part1', [])\n",
        "\n",
        "# Check if the list was loaded correctly\n",
        "print(f\"Loaded {len(scientific_facts_part1)} sentences.\")\n",
        "print(scientific_facts_part1[:5])  # Display the first 5 sentences\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68JGpYZ9Z9jQ",
        "outputId": "2d57f94a-0ba8-4f98-d0f7-d09cf4b85773"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 178 sentences.\n",
            "['The Earth revolves around the Sun.', 'Water is made up of two hydrogen atoms and one oxygen atom.', 'Humans have 23 pairs of chromosomes.', 'The force of gravity keeps us on the ground.', 'Plants produce oxygen through a process called photosynthesis.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize CountVectorizer to convert text into numerical format\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Split each sentence into input (all words except the last) and output (the last word)\n",
        "input_data = []\n",
        "output_data = []\n",
        "\n",
        "for sentence in scientific_facts_part1:\n",
        "    words = sentence.split()\n",
        "    if len(words) > 1:  # Ensure that there's at least one word to predict\n",
        "        input_data.append(' '.join(words[:-1]))  # All words except the last\n",
        "        output_data.append(words[-1])            # The last word\n",
        "\n",
        "# Verify that input and output data are prepared correctly\n",
        "print(\"Sample input data:\", input_data[:3])  # Check a few input sentences\n",
        "print(\"Sample output data:\", output_data[:3])  # Check corresponding target words\n",
        "\n",
        "# Convert input sentences into feature vectors\n",
        "X = vectorizer.fit_transform(input_data).toarray()\n",
        "# Transform output data into numerical format using a simple approach\n",
        "# For SVM, we will convert the target words to integer indices\n",
        "unique_words = list(set(output_data))\n",
        "word_to_index = {word: idx for idx, word in enumerate(unique_words)}\n",
        "y = np.array([word_to_index[word] for word in output_data])\n",
        "\n",
        "# Convert output words into feature vectors\n",
        "# y = vectorizer.transform(output_data).toarray()\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check vectorizer vocabulary\n",
        "print(\"Vectorizer vocabulary size:\", len(vectorizer.vocabulary_))\n",
        "print(\"Sample words from vocabulary:\", list(vectorizer.vocabulary_.keys())[:10])\n",
        "\n",
        "# Ensure correct transformation of labels\n",
        "sample_word = output_data[0]  # Take a sample word from output data\n",
        "encoded_sample = vectorizer.transform([sample_word]).toarray()  # Encode it\n",
        "decoded_sample = vectorizer.inverse_transform(encoded_sample)  # Decode it back\n",
        "print(f\"Encoded sample: {encoded_sample}, Decoded sample: {decoded_sample}\")\n"
      ],
      "metadata": {
        "id": "YUuyC7O_g4U1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "961ab251-69bd-4231-dab8-95f73f476f9c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample input data: ['The Earth revolves around the', 'Water is made up of two hydrogen atoms and one oxygen', 'Humans have 23 pairs of']\n",
            "Sample output data: ['Sun.', 'atom.', 'chromosomes.']\n",
            "Vectorizer vocabulary size: 597\n",
            "Sample words from vocabulary: ['the', 'earth', 'revolves', 'around', 'water', 'is', 'made', 'up', 'of', 'two']\n",
            "Encoded sample: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]], Decoded sample: [array(['sun'], dtype='<U16')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the SVM model with a linear kernel\n",
        "svm_model = SVC(kernel='linear', probability=True)\n",
        "\n",
        "# Train the SVM model on the training data\n",
        "# svm_model.fit(X_train, y_train.argmax(axis=1))\n",
        "svm_model.fit(X, y)\n",
        "\n",
        "# Test the SVM model predictions on a sample input\n",
        "test_sentence = \"The process of breaking down food into energy in the human body is called \"  # Example test sentence\n",
        "test_vector = vectorizer.transform([test_sentence]).toarray()\n",
        "# print(\"Test Vector:\", test_vector)\n",
        "\n",
        "\n",
        "# Test the SVM model's predictions on the test set\n",
        "svm_pred = svm_model.predict(test_vector)\n",
        "\n",
        "# Correctly interpret the predicted index\n",
        "predicted_index = svm_pred[0]\n",
        "\n",
        "# Map the predicted index back to the corresponding word using the index-to-word mapping\n",
        "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
        "\n",
        "# Check if the predicted index is valid\n",
        "if predicted_index in index_to_word:\n",
        "    predicted_word_svm = index_to_word[predicted_index]\n",
        "    print(\"SVM Prediction:\", predicted_word_svm)\n",
        "else:\n",
        "    print(\"Predicted index is out of range.\")\n",
        "\n",
        "# Predict using the SVM model\n",
        "svm_pred = svm_model.predict(test_vector)\n",
        "print(\"Predicted Index by SVM:\", svm_pred)\n",
        "\n",
        "# # Calculate and print the accuracy of the SVM model\n",
        "# svm_accuracy = accuracy_score(y_test.argmax(axis=1), svm_predictions)\n",
        "# print(\"SVM Accuracy:\", svm_accuracy)"
      ],
      "metadata": {
        "id": "WFoDr_uLheRk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "186b386f-1acd-4c72-beef-986c99fd119c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Prediction: metabolism.\n",
            "Predicted Index by SVM: [87]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Neural network\n",
        "import numpy as np\n",
        "# Step 1: Prepare the tokenizer and encode the sentences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(scientific_facts_part1)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Create sequences for training (predicting only the last word)\n",
        "input_sequences = []\n",
        "target_words = []\n",
        "\n",
        "for line in scientific_facts_part1:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    if len(token_list) > 1:  # Ensure there is more than one word in the sentence\n",
        "        input_sequences.append(token_list[:-1])  # All words except the last one\n",
        "        target_words.append(token_list[-1])  # Only the last word\n",
        "\n",
        "# Pad input sequences to the same length\n",
        "max_sequence_len = max(len(x) for x in input_sequences)\n",
        "X = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "# Convert the target words into categorical format\n",
        "y = to_categorical(target_words, num_classes=total_words)\n",
        "\n",
        "# Step 2: Define a robust neural network model\n",
        "nn_model = Sequential([\n",
        "    Embedding(total_words, 128),  # Embedding layer\n",
        "    LSTM(128),  # Single LSTM layer\n",
        "    Dense(total_words, activation='softmax')  # Output layer\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "nn_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.005), metrics=['accuracy'])\n",
        "\n",
        "# Step 3: Add early stopping and train the model\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "history = nn_model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "# Step 4: Testing the model with a sample input\n",
        "test_sentence = \"The Earth revolves around the\"\n",
        "test_sequence = tokenizer.texts_to_sequences([test_sentence])[0]\n",
        "test_padded = pad_sequences([test_sequence], maxlen=max_sequence_len, padding='pre')\n",
        "prediction = np.argmax(nn_model.predict(test_padded), axis=-1)\n",
        "predicted_word = tokenizer.index_word[prediction[0]]\n",
        "\n",
        "print(\"Neural Network Prediction:\", predicted_word)"
      ],
      "metadata": {
        "id": "bHZVROv6lGgR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4af9de91-f68f-4631-b1cd-bb61a1a2eec7"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 220ms/step - accuracy: 0.0000e+00 - loss: 6.5312 - val_accuracy: 0.0278 - val_loss: 6.3918\n",
            "Epoch 2/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - accuracy: 0.0262 - loss: 5.7161 - val_accuracy: 0.0278 - val_loss: 7.0815\n",
            "Epoch 3/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - accuracy: 0.0213 - loss: 4.8727 - val_accuracy: 0.0278 - val_loss: 8.1111\n",
            "Epoch 4/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - accuracy: 0.0327 - loss: 4.6808 - val_accuracy: 0.0278 - val_loss: 8.7362\n",
            "Epoch 5/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.0306 - loss: 4.6994 - val_accuracy: 0.0278 - val_loss: 9.0693\n",
            "Epoch 6/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.0362 - loss: 4.6456 - val_accuracy: 0.0278 - val_loss: 9.1477\n",
            "Epoch 7/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.0425 - loss: 4.5740 - val_accuracy: 0.0278 - val_loss: 9.1032\n",
            "Epoch 8/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.0662 - loss: 4.5295 - val_accuracy: 0.0556 - val_loss: 8.8282\n",
            "Epoch 9/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.1014 - loss: 4.3455 - val_accuracy: 0.0556 - val_loss: 8.5539\n",
            "Epoch 10/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1282 - loss: 4.1816 - val_accuracy: 0.0556 - val_loss: 8.3475\n",
            "Epoch 11/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.1343 - loss: 4.0717 - val_accuracy: 0.0833 - val_loss: 8.2639\n",
            "Epoch 12/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.1731 - loss: 3.8174 - val_accuracy: 0.0556 - val_loss: 8.1179\n",
            "Epoch 13/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2123 - loss: 3.6014 - val_accuracy: 0.0556 - val_loss: 8.1770\n",
            "Epoch 14/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.2524 - loss: 3.3309 - val_accuracy: 0.0833 - val_loss: 7.9751\n",
            "Epoch 15/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2851 - loss: 3.1977 - val_accuracy: 0.0833 - val_loss: 8.1505\n",
            "Epoch 16/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.3562 - loss: 2.8926 - val_accuracy: 0.1389 - val_loss: 8.2368\n",
            "Epoch 17/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.4180 - loss: 2.6913 - val_accuracy: 0.1667 - val_loss: 8.2205\n",
            "Epoch 18/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.5879 - loss: 2.4043 - val_accuracy: 0.1111 - val_loss: 8.5736\n",
            "Epoch 19/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.6110 - loss: 2.2003 - val_accuracy: 0.1389 - val_loss: 8.4772\n",
            "Epoch 20/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7361 - loss: 1.8651 - val_accuracy: 0.0833 - val_loss: 8.8697\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step\n",
            "Neural Network Prediction: sun\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Make predictions with the neural network\n",
        "# nn_pred = nn_model.predict(test_vector_padded)\n",
        "# predicted_index = np.argmax(nn_pred, axis=1)[0]\n",
        "\n",
        "# # Map the predicted index to the corresponding word in the filtered output vocabulary\n",
        "# predicted_word_nn = index_to_output_word.get(predicted_index, \"Unknown\")\n",
        "# print(\"Neural Network Prediction:\", predicted_word_nn)\n",
        "def predict_last_word(sentence):\n",
        "    # Tokenize and pad the sentence\n",
        "    token_list = tokenizer.texts_to_sequences([sentence])[0]\n",
        "    token_list_padded = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\n",
        "    # Make prediction for the sentence\n",
        "    predicted = nn_model.predict(token_list_padded)\n",
        "    predicted_index = np.argmax(predicted, axis=-1)[0]\n",
        "\n",
        "    # Map the predicted index to the corresponding word\n",
        "    predicted_word = tokenizer.index_word.get(predicted_index, \"Unknown\")\n",
        "\n",
        "    return predicted_word\n",
        "\n",
        "# Example sentences to test\n",
        "test_sentences = [\n",
        "    \"The Earth revolves around the\",\n",
        "    \"Water is made up of two hydrogen\",\n",
        "    \"Humans have 23 pairs of\",\n",
        "    \"The chemical formula for table salt is\",\n",
        "    \"DNA stands for deoxyribonucleic\",\n",
        "    \"The largest planet in our solar system is\",\n",
        "    \"The process of converting light energy into chemical energy is\",\n",
        "    \"The greenhouse effect traps heat in the Earth's\",\n",
        "    \"A herbivore is an animal that feeds on\",\n",
        "    \"A parallelogram is a four-sided shape with opposite sides that are\",\n",
        "    \"The human respiratory system is responsible for breathing and gas\"\n",
        "]\n",
        "\n",
        "# Predict the last word for each sentence\n",
        "for sentence in test_sentences:\n",
        "    predicted_word = predict_last_word(sentence)\n",
        "    print(f\"Sentence: '{sentence}' -> Predicted Last Word: '{predicted_word}'\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1Fjtbsc41sG8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e58758b8-ca62-40e7-b4a7-6bc1a0b92ad8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step\n",
            "Sentence: 'The Earth revolves around the' -> Predicted Last Word: 'sun'\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "Sentence: 'Water is made up of two hydrogen' -> Predicted Last Word: 'atom'\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "Sentence: 'Humans have 23 pairs of' -> Predicted Last Word: 'chromosomes'\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "Sentence: 'The chemical formula for table salt is' -> Predicted Last Word: 'nitrogen'\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Sentence: 'DNA stands for deoxyribonucleic' -> Predicted Last Word: 'acid'\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Sentence: 'The largest planet in our solar system is' -> Predicted Last Word: 'oxygen'\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "Sentence: 'The process of converting light energy into chemical energy is' -> Predicted Last Word: 'photosynthesis'\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "Sentence: 'The greenhouse effect traps heat in the Earth's' -> Predicted Last Word: 'atmosphere'\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "Sentence: 'A herbivore is an animal that feeds on' -> Predicted Last Word: 'plants'\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "Sentence: 'A parallelogram is a four-sided shape with opposite sides that are' -> Predicted Last Word: 'parallel'\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "Sentence: 'The human respiratory system is responsible for breathing and gas' -> Predicted Last Word: 'exchange'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a sample of the target words to verify correctness\n",
        "print(\"Sample output words:\", output_data)\n",
        "\n",
        "# Ensure the set of unique output words matches your expectations\n",
        "unique_output_words = set(output_data)\n",
        "print(\"Unique output words:\", unique_output_words)"
      ],
      "metadata": {
        "id": "PaJy_wAN3AGG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbefd765-f82d-44f3-ef9f-1cad77a93c4a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample output words: ['Sun.', 'atom.', 'chromosomes.', 'ground.', 'photosynthesis.', 'second.', 'water.', 'oxygen.', 'cell.', 'NaCl.', 'gas.', 'System.', 'level.', 'acid.', 'Jupiter.', 'oxygen.', 'days.', 'bones.', 'sound.', 'cell.', 'particles.', 'old.', 'tissues.', 'system.', 'hydrogen.', 'nickel.', 'Sun.', 'chlorophyll.', 'oxygen.', 'year.', 'neurons.', 'gravitation.', 'blue.', 'temperature.', 'is.', 'matter.', 'element.', 'Planet.', 'system.', 'atoms.', 'charge.', 'Einstein.', \"'Au'.\", 'light.', 'photosynthesis.', 'phenomenon.', 'Sun.', 'microorganisms.', 'angles.', 'Earth.', 'obscured.', 'number.', 'fur.', 'solute.', 'cells.', 'colors.', 'mitosis.', '2006.', 'compound.', 'environment.', 'phenomena.', 'earthquakes.', 'motion.', 'reaction.', 'atmosphere.', 'ammonia.', 'insulators.', 'point.', 'world.', 'plants.', 'minerals.', 'atmosphere.', 'equation.', 'carbon.', 'years.', 'orbit.', 'chambers.', 'mass.', 'transpiration.', 'Way.', 'nucleus.', 'gases.', 'animals.', 'space.', 'power.', 'metals.', 'photosynthesis.', 'oceans.', 'pentagon.', 'world.', 'space.', 'interact.', 'plants.', 'ecosystem.', 'animals.', 'universe.', 'parallel.', 'nitrogen.', 'Australia.', 'escape.', 'electrons.', 'metabolism.', 'world.', 'particles.', 'length.', 'Egyptians.', 'consumed.', 'photosynthesis.', 'world.', 'botany.', 'iron.', 'evaporation.', 'polygon.', 'Moon.', 'environment.', 'sublimation.', 'Moon.', 'plates.', 'pH.', 'silver.', 'condensation.', 'spoon.', 'plants.', 'freezing.', 'substance.', 'electricity.', 'nutrients.', 'Sun.', 'universe.', 'melting.', 'organisms.', 'spoon.', 'exchange.', 'disease.', 'time.', 'pH.', 'reproduction.', 'matter.', 'core.', 'edges.', 'plants.', 'photosynthesis.', 'America.', 'animals.', 'potassium.', 'deposition.', 'view.', 'plants.', 'nutrition.', 'interaction.', 'element.', 'substances.', 'shadow.', 'weather.', 'sublimation.', 'plants.', 'edges.', 'body.', '(weeds).', 'surroundings.', '7.', 'photosynthesis.', 'materials.', 'Epoch.', 'means.', 'tissues.', 'Sun.', 'Earth.', 'diffusion.', 'organisms.', 'nearsightedness.', 'system.', 'plants.', 'present.', 'compound.', 'pH.', 'Moon.', 'whole.']\n",
            "Unique output words: {'system.', 'matter.', 'phenomena.', 'means.', 'ammonia.', 'is.', 'nutrition.', 'years.', 'tissues.', 'freezing.', 'space.', 'deposition.', 'gravitation.', 'disease.', 'diffusion.', 'consumed.', 'element.', 'ecosystem.', 'NaCl.', 'metals.', 'carbon.', 'Egyptians.', 'time.', 'light.', 'reaction.', 'phenomenon.', 'insulators.', 'year.', 'ground.', 'orbit.', 'equation.', 'days.', 'escape.', 'Way.', 'obscured.', 'compound.', '7.', 'core.', 'colors.', 'body.', 'exchange.', '2006.', 'chlorophyll.', 'Epoch.', 'oceans.', 'pentagon.', 'level.', \"'Au'.\", 'mitosis.', 'Australia.', 'materials.', 'fur.', 'bones.', 'blue.', 'substance.', 'interaction.', 'temperature.', 'shadow.', 'acid.', 'Planet.', 'earthquakes.', 'botany.', 'reproduction.', 'angles.', 'chromosomes.', 'chambers.', 'atoms.', 'nutrients.', 'condensation.', 'environment.', 'plants.', 'plates.', 'nucleus.', 'number.', 'point.', 'pH.', 'electricity.', 'iron.', 'organisms.', 'gases.', 'solute.', 'universe.', 'nitrogen.', 'System.', 'whole.', 'weather.', 'present.', 'metabolism.', 'sublimation.', 'surroundings.', 'mass.', 'nickel.', 'atmosphere.', 'silver.', 'Sun.', 'length.', 'atom.', 'parallel.', 'Jupiter.', 'photosynthesis.', '(weeds).', 'charge.', 'old.', 'transpiration.', 'power.', 'Earth.', 'sound.', 'hydrogen.', 'melting.', 'substances.', 'neurons.', 'polygon.', 'animals.', 'potassium.', 'gas.', 'Moon.', 'America.', 'evaporation.', 'microorganisms.', 'motion.', 'cells.', 'world.', 'interact.', 'second.', 'cell.', 'Einstein.', 'view.', 'electrons.', 'minerals.', 'spoon.', 'edges.', 'water.', 'nearsightedness.', 'oxygen.', 'particles.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sUw-JbQNKDne"
      },
      "execution_count": 44,
      "outputs": []
    }
  ]
}